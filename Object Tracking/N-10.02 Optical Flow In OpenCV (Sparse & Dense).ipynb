{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font style=\"color:rgb(100,109,254)\"> Optical Flow </font> </center>\n",
    "Optical flow is a motion estimation algorithm. This will help you figure out apparent motion of image objects between two consecutive frames caused by either movement of object or camera. \n",
    "\n",
    "Optical flow has many applications in areas like:\n",
    "\n",
    "- Object Tracking\n",
    "\n",
    "- Structure from Motion (SFM)\n",
    "\n",
    "- Video Compression\n",
    "\n",
    "- Visual Odometry \n",
    "\n",
    "- Video Stabilization.\n",
    "\n",
    "\n",
    "**Following two constraints must be taken into account while using Optical Flow:**\n",
    "\n",
    "1. The pixel intensities of an object do not change between consecutive frames.\n",
    "2. Neighbouring pixels have similar motion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(134,19,348)\">  Tracking Points with Lucas-Kanade Optical Flow.  </font>\n",
    "Now we are going to try Lucas-Kanade version of Optical flow, which is a sparse Optical flow implementation. Since we are just going to track some points in a video, we should use good feature points and some good points to track with Optical flow are corners, so we're going to use `cv2.goodFeaturesToTrack()` to get corners and then track them using the function `cv2.calcOpticalFlowPyrLK()`.\n",
    "\n",
    "[```nextPts, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria]]]]])```](https://docs.opencv.org/4.2.0/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323)\n",
    "\n",
    "Params:\n",
    "\n",
    "- **`prevImg`**\tfirst 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
    "- **`nextImg`**\tsecond input image or pyramid of the same size and the same type as prevImg.\n",
    "- **`prevPts`**\tvector of 2D points for which the flow needs to be found; point coordinates must be single-precision floating-point numbers.\n",
    "- **`nextPts`**\toutput vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n",
    "- **`status`**\toutput status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "err\toutput vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't found then the error is not defined (use the status parameter to find such cases).\n",
    "- **`winSize`**\tsize of the search window at each pyramid level.\n",
    "- **`maxLevel`**\t0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
    "- **`criteria`**\tparameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a Sample Video\n",
    "cap = cv2.VideoCapture('media/M4/slow.mp4')\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),  maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Take first frame as the initial frame and find corners (good point to track) in it\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Convert to grayscale\n",
    "previous_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Get the corner points\n",
    "old_points = cv2.goodFeaturesToTrack(previous_frame, mask = None, maxCorners = 200, qualityLevel = 0.2, minDistance = 5,\n",
    "blockSize = 7)\n",
    "\n",
    "# Create a black image, this will be our drawing Canvas.\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    current_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate optical flow\n",
    "    new_points, status, err = cv2.calcOpticalFlowPyrLK(previous_frame, current_frame, old_points, None, **lk_params)\n",
    "   \n",
    "    # Select good points based on the status\n",
    "    new_good_points = new_points[status==1]\n",
    "    old_good_points = old_points[status==1]\n",
    "\n",
    "    for (new, old) in zip(new_good_points, old_good_points):\n",
    "        a,b =  new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        # Draw a line from x1,y1 to x2,y2\n",
    "        mask = cv2.line(mask, (a,b),(c,d), [0,0,255], 2)\n",
    "        # Circle the tracked Corner Points\n",
    "        frame = cv2.circle(frame,(a,b),5,[255,0,0],-1)     \n",
    "        \n",
    "    # Combine the mask and frame to display the drawing.    \n",
    "    combined = cv2.add(frame,mask)\n",
    "    cv2.imshow('frame', combined)\n",
    "\n",
    "    k = cv2.waitKey(30) \n",
    "    if k == 27:\n",
    "        break\n",
    "        \n",
    "    # At the end current frame becomes the previous frame for the next iteration\n",
    "    previous_frame = current_frame.copy()\n",
    "    \n",
    "    # At the end new points become the old points for the next iteration\n",
    "    old_points = new_good_points.reshape(-1,1,2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(134,19,348)\">   Tracking a Single Point with Optical Flow </font>\n",
    "With below script you can choose a single point to track with the mouse on a webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    " \n",
    "_, frame = cap.read()\n",
    "frame = cv2.flip(frame, 1) \n",
    "\n",
    "prev_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# Lucas kanade params\n",
    "lk_params = dict(winSize = (10, 10), maxLevel = 4,  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    " \n",
    "# This function will let user select a point to track.\n",
    "def select_point(event, x, y, flags, params):\n",
    "    global point, point_selected, old_point\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        point = (x, y)\n",
    "        point_selected = True\n",
    "        old_point = np.array([[x, y]], dtype=np.float32).reshape(-1,1,2)\n",
    " \n",
    "cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "cv2.setMouseCallback(\"Frame\", select_point)\n",
    "\n",
    "# By default this would be False.\n",
    "point_selected = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip( frame, 1 ) \n",
    "    current_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # If a point was selected by the mouse then proceed.\n",
    "    if point_selected:\n",
    "        #cv2.circle(frame, point, 5, (0, 0, 255), 2)\n",
    "\n",
    "        new_point, status, error = cv2.calcOpticalFlowPyrLK(prev_frame, current_frame, old_point, None, **lk_params)\n",
    "        \n",
    "        if status[0][0] > 0:\n",
    "        \n",
    "            new_good_point = new_point[status==1]\n",
    "\n",
    "            x, y = new_good_point.ravel()\n",
    "            cv2.circle(frame, (x, y), 10, (0, 0, 255), -1)\n",
    "\n",
    "            prev_frame = current_frame\n",
    "            old_point = new_good_point.reshape(-1,1,2)\n",
    "        else:\n",
    "            cv2.putText(frame,'Lost Point',(10,50), cv2.FONT_HERSHEY_COMPLEX, 1, (20,25,150), 2, cv2.LINE_AA)\n",
    " \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(134,19,348)\"> Dense Optical Flow in OpenCV   </font>\n",
    "Lucas-Kanade method computes optical flow for a sparse feature set (which we used as corners). OpenCV provides another algorithm to find dense optical flow. It computes the optical flow for all the points in the frame. It is based on **Gunner Farneback's** algorithm which is explained in [**`Two-Frame Motion Estimation Based on Polynomial Expansion\" by Gunner Farneback in 2003.`**](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf)\n",
    "\n",
    "**You can read more about this by reading the research paper linked above**\n",
    "\n",
    "Below python script shows how to find the dense optical flow using Gunner's algorithm. We get a 2-channel array with optical flow vectors, (u,v) using the function **cv2.calcOpticalFlowFarneback()**. We then find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane. Saturation is kept maximum (255) for maximizing visibility.\n",
    "\n",
    "[```flow\t= cv2.calcOpticalFlowFarneback(\tprev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)```](https://docs.opencv.org/4.2.0/dc/d6b/group__video__track.html#ga5d10ebbd59fe09c5f650289ec0ece5af)\n",
    "\n",
    "**Params:**\n",
    "\n",
    "- **`prev`**\tfirst 8-bit single-channel input image.\n",
    "- **`next`**\tsecond input image of the same size and the same type as prev.\n",
    "- **`flow`**\tcomputed flow image that has the same size as prev and type CV_32FC2.\n",
    "- **`pyr_scale`**\tparameter, specifying the image scale (<1) to build pyramids for each image; pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.\n",
    "- **`levels`**\tnumber of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.\n",
    "- **`winsize`**\taveraging window size; larger values increase the algorithm robustness to image noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    "iterations\tnumber of iterations the algorithm does at each pyramid level.\n",
    "- **`poly_n`**\tsize of the pixel neighborhood used to find polynomial expansion in each pixel; larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    "- **`poly_sigma`**\tstandard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5.\n",
    "- **`flags`**\toperation flags that can be a combination of the following:\n",
    "\n",
    "  1. **OPTFLOW_USE_INITIAL_FLOW** uses the input flow as an initial flow approximation.\n",
    "  2. **OPTFLOW_FARNEBACK_GAUSSIAN** uses the Gaussian winsizeÃ—winsize filter instead of a box filter of the same size for optical flow estimation; usually, this option gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for a Gaussian window should be set to a larger value to achieve the same level of robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a sample video\n",
    "cap = cv2.VideoCapture(\"media/M4/vtest.avi\")\n",
    "ret, frame = cap.read()\n",
    "\n",
    "prev_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Our mask in which we will color code.\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "# The saturation in final result must be maximum.\n",
    "mask[...,1] = 255\n",
    "\n",
    "while(1):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    current_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Using Farneback's Optical Flow, this computes the magnitude and the angles of 2d Vectors\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_frame, current_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # The First channel corresponds to the magnitude and the second to angle \n",
    "    magnitude, angle = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    \n",
    "    # Setting the hue according to the optical flow direction.\n",
    "    mask[...,0] = angle*180/np.pi/2\n",
    "    # Setting the Value channel to the optical flow magnitude normalized between 0-255\n",
    "    mask[...,2] = cv2.normalize(magnitude,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Converting hsv to bgr so we can display the image with imshow.\n",
    "    bgr = cv2.cvtColor(mask,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Stack the original frame with the result\n",
    "    final = cv2.addWeighted(frame,0.5, bgr, 0.5, 0)\n",
    "    \n",
    "    cv2.imshow('frame', final)\n",
    "    k = cv2.waitKey(1) \n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # At the end current frame becomes the previous frame for the next iteration.\n",
    "    prev_frame = current_frame\n",
    "\n",
    "                \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
